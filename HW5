1-)
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

# Dataset
X, y = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)

# Scatter plot
plt.figure(figsize=(6, 6))
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')

# Graph
plt.title("Non-linearly Separable Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.show()



2-)
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from sklearn.svm import SVC
import numpy as np
from sklearn.metrics import accuracy_score

# Dataset 
X, y = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)

# Linear SVM model
model = SVC(kernel='linear', C=1.0)
model.fit(X, y)

# Meshgrid for decision function
xx, yy = np.meshgrid(
    np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 500),
    np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 500)
)
Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Calculating margin size
w = model.coef_[0]
margin = 2 / np.linalg.norm(w)
print(f"Margin size: {margin:.4f}")

# Missclassification rate 
y_pred = model.predict(X)
accuracy = accuracy_score(y, y_pred)
errors = (y != y_pred).sum()

print(f"Accuracy on training data: {accuracy * 100:.2f}%")
print(f"Number of misclassified points: {errors}")

# Plotting
plt.figure(figsize=(6, 6))
plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='black')
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')
plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
            s=100, facecolors='none', edgecolors='black', label='Support Vectors')

plt.title("Linear SVM on Non-linearly Separable Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.show()



3-)
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from sklearn.svm import SVC
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

# Dataset 
X, y = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)

# Non-linear SVM (RBF kernel)
model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X, y)

# Creating meshgrid for decision function
x1 = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 100)
x2 = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 100)
xx1, xx2 = np.meshgrid(x1, x2)
X_grid = np.c_[xx1.ravel(), xx2.ravel()]

# Computing decision function values
Z = model.decision_function(X_grid)
Z = Z.reshape(xx1.shape)

# 3D Plot
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Plotting decision surface
ax.plot_surface(xx1, xx2, Z, cmap='coolwarm', alpha=0.6, linewidth=0)

# Plotting original data (z=0)
ax.scatter(X[y == 0][:, 0], X[y == 0][:, 1], 0, color='red', label='Class 0')
ax.scatter(X[y == 1][:, 0], X[y == 1][:, 1], 0, color='blue', label='Class 1')

ax.set_title("Q3: 3D Decision Surface of RBF Kernel SVM")
ax.set_xlabel("Feature 1")
ax.set_ylabel("Feature 2")
ax.set_zlabel("Decision Function Value")
ax.view_init(elev=30, azim=135)  # rotate view
plt.legend()
plt.tight_layout()
plt.show()


4-)
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from sklearn.svm import SVC
import numpy as np

# Dataset 
X, y = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)

# Training RBF kernel SVM 
model = SVC(kernel='rbf', C=1.0, gamma='scale')
model.fit(X, y)

# Creating meshgrid for decision function
x1 = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 500)
x2 = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 500)
xx1, xx2 = np.meshgrid(x1, x2)
X_grid = np.c_[xx1.ravel(), xx2.ravel()]
Z = model.decision_function(X_grid)
Z = Z.reshape(xx1.shape)

# Plotting decision boundary
plt.figure(figsize=(7, 7))
plt.contour(xx1, xx2, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='black')

# Ploting data points
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')

# Marking support vectors
plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
            s=100, facecolors='none', edgecolors='black', label='Support Vectors')

# Graph formatting
plt.title("Q4: 2D Decision Boundary with RBF Kernel SVM")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



